# Amazing LLM

## 背景描述

> “一觉醒来，全世界的HPC水平下降100万倍而只有我的不变？”

你穿越了，穿越到一个全世界的HPC水平急剧降低的世界，而你却保留了原来的所有知识。你惊讶的发现人们居然使用了 `6bit` 这种不可思议的量化方式对LLM的权重进行量化。由于缺乏优化现在大家的LLM的推理效率极低无比。好在和你一起穿越的电脑里面存了一个来自于旧世界的名为 [`vllm`](https://github.com/vllm-project/vllm) 的推理框架的源代码，你决定使用它帮助这个世界的人们提升LLM的推理效率。但是你发现这个世界里面的模型权重全部存储成了`6bit`量化的形式，无法直接运行推理框架。于是你决定修改代码使得 `vllm` 可以从 `6bit` 量化的权重中载入进行推理。

## 任务描述

### 编译运行 `vllm`

1. 编译安装题目给出的 `vllm` 推理框架（源码压缩包可运行 `download.sh` 下载到 `source_code` 文件夹中）
2. 给出的源码压缩包已进行部分修改，其在载入权重的部分将调用 `q6bit` 库中的 `recover_from_quant` 函数进行解量化。自定义库 `q6bit` 的 `baseline` 代码已给出，你需要自行安装到环境中。完成安装后运行 `source_code/run.py` 成功启动服务。启动服务成功后可运行 `source_code/test_server.py` 向服务发送请求查看回复，同时该脚本将会把回复存储到 `ans.txt` 供你提交。

> `source_code/run.py` 与 `source_code/test_server.py` 均可以通过 `-port` 参数指定 `vllm` 运行的端口，注意 `run.py` 会检测指定的端口是否占用，如果占用将尝试寻找空闲端口， 传入 `test_server.py` 的端口号需与 `vllm` 的实际运行端口一致。这两个脚本的默认运行端口均为 `18000`。

### 优化解量化函数

优化 `vllm` 推理框架中调用的 `q6bit` 库中的 `recover_from_quant` 函数。在进行优化时你仅可以修改 `q6bit` 库的代码（`source_code/quant`），且仅允许做以下修改：
* 添加/修改/删除 `csrc` 文件夹下的 `.cpp`,`.cc`,`.c`,`.h`,`.hpp` 文件，不允许新建子文件夹
* 添加/修改/删除 `q6bit` 文件夹下的 `.py` 文件，不允许新建子文件夹
* 修改 `setup.json` 文件

> 你也可以运行`python submit.py`打包你的提交文件，检查你修改的文件是否打包进入了提交文件中，如果未打包进入提交文件则表明你对此文件的修改无效。

此库编译时环境由 `pyproject.toml` 给出，不允许更改。此库的运行时环境由给出的 `vllm` 代码安装后的环境决定，不允许引入新的依赖库。

你需要保证优化前后解量化得到的数据完全一致，优化后的正确性校验将由两个位置给出：
* 进行解量化后将抽样计算哈希值，如果哈希值发生变化则正确性验证不通过
* 解量化完成后进行服务时，使用 `source_code/test_server.py` 发送相同的请求应得到一致的回复，如果回复内容出现变化则正确性验证不通过


## 评分标准

完成任务描述的 “编译运行 `vllm`” 阶段，成功运行服务后提交指定输入的运行结果 `ans.txt` 即可得到 `30%` 的分数。剩余 `70%` 的分数按照解量化的速度给出，你的得分由你的解量化函数运行时间 `ti` 在满分时间 `t_full` 和零分时间 `t_zero` 对数线性插值得到，即：

$$ your\_score = \frac{\ln(t\_zero) - \ln(ti)}{\ln(t\_zero) - \ln(t\_full)} $$

其中满分时间为 0.9s，零分时间为 6.2s。`baseline` 的运行时间在 7s 左右，直接提交`baseline`将不会得到任何分数。
在我们给出的 `vllm` 源码中的 `vllm/model_executor/model_loader/weight_utils.py` 文件的 490 - 502 行已经写好了计时的逻辑，运行时将出现下述输出：
``` txt
==================== begin to time dequant ====================
100%|████████████████████████████████████████████████████████████████████| 254/254 [00:07<00:00, 36.13it/s]
dequant cost 7.0764 s
cal hash cost 0.0039 s  sha256: 6d46f62caef0d415ade2ba0f5d1f1178af39332fba692008c0846417c66550bb
==================== end to time dequant ====================
```
其中 "dequant cost" 后面即为测量的解量化时间。

**本题的核心数限制为32，每核心配比的内存大小为1950MB。在给出的脚本中均已经设置好了核心数的限制。**

## 提交文件

题目中附带了 `submit.py` 供你一键打包提交文件，用法如下：
```bash
python submit.py <提交的stage> 
# 0代表编译运行vllm，1代表编译运行+优化解量化。默认值为0
# 示例：
python submit.py 0 # 提交阶段0的文件
python submit.py 1 # 提交阶段1的文件
# 注意评测机在实际评测时只有编译运行vllm的文件校验通过后才评测优化解量化
```
## 自测脚本

题目中附带了 `evaluate.py` 模拟OJ的评测逻辑供你进行自测，其依赖项为 `vllm` 的依赖项的子集，设计为在你安装完成 `vllm` 的环境下使用，用法如下：
``` bash
python evaluate.py --stage <使用的stage> --port <vllm运行使用的端口>
# 这里stage的含义与提交文件脚本中stage含义一致
# 示例：
python evaluate.py # 默认的stage为0,此时只评测编译运行vllm部分
python evaluate.py --stage 1 # 评测stage为1（即评测本题目所有内容）
```
在评测机测试过程中，解量化时间的计算方式如下：在完成一次预热操作后，取后续三次测试结果的平均值作为最终的解量化时间（记为 $t_{avg}$）。同时，为保障测试性能稳定性，明确要求冷启动时间不得超过阈值，该阈值按 $\max (t_{avg} \times 1.5, t_{avg}+1.5)$ 的公式计算得出。（此计算逻辑已在给出的 `evaluate.py` 实现）

## 提示

* vllm提供了较为完善的[文档](https://docs.vllm.ai)可供参考。
* `baseline` 使用的是 `pytorch` 原生操作完成的解量化。`pytorch` 提供了使用 `C++` 撰写自定义算子的接口，在 `q6bit` 库中给出了 `q6bit.cpp`，你可以使用 `C++` 自行编写解量化算子以获得更高的性能，绑定到 `Python` 函数的代码已经帮你写好（见 `cfunc.py`）。
* 在 `q6bit` 库中写有 `speed_test.py` 脚本可以用于帮助你快速测试当前库的 `recover_from_quant` 函数的性能。
* 评测环境说明
    - **Python 版本**：3.12
    - **C/C++ 编译器及版本**：GCC/G++ 14.3.0
    
    为避免因环境差异导致未知兼容性问题，建议在本地开发过程中使用与上述一致的版本配置。